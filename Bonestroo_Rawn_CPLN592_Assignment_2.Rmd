---
title: "CPLN 592 Assignment 2"
author: "Hannah Bonestroo and Brian Rawn"
date: "10/16/2020"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---
<style>

table, td, th {
  border: none;
  padding-left: 1em;
  padding-right: 1em;
  margin-left: auto;
  margin-right: auto;
  margin-top: 1em;
  margin-bottom: 1em;
}

</style>

# Hedonic Home Price Prediction in Miami, Florida

***
# 1.1 Executive Summary

## Motivation

The question of "what is a home worth?" has a simple answer: whatever a buyer is willing to pay. But predicting what a buyer is willing to pay before a sale has occurred is no easy feat. Houses are not commodity goods. The many attributes that make a home valuable, everything from the number of bedrooms to the amenities of the neighborhood it is located in, vary significantly from home to home. Further complicating the problem is the fact that these characteristics are valued differently across different types of buyers, neighborhoods, and markets. This model attempts to address some of the challenges inherent in predicting home prices by utilizing the power of ordinary least squares regression and the insight offered by local spatial data. 

## Process

Our process consisted of first gathering available datasets that held promise as predictors of home value. After wrangling the data, we evaluated each variable against several statistical criteria. After testing many combinations, we included in our final model a set of variables that most effectively minimized mean average error (MAE) while attempting to be as accurate and generalizable as possible.

## Results

Although we evaluated many variables, we found that a simple model including only four features minimized MAE when predicting sale prices. Including more features resulted often in higher r-squared values, but also in higher MAE, suggesting that a more simple model is most effective for minimizing error. Nonetheless, our model is far from a perfect home price predictor and exhibits several problematic characteristics, including limited generalizability, that will be discussed below.


```{r setup, results='hide', error=FALSE, message=FALSE, warning=FALSE}
setwd("C:/Users/Hannah/Documents/Penn/Fall 2020/CPLN-592/Assignments/Assignment 2/CPLN592Assignment2")
library(tidyverse)
library(tidycensus)
library(kableExtra)
library(tidycensus)
library(sf)
library(gridExtra)
library(grid)
library(knitr)
library(rmarkdown)
library(ggcorrplot)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(rgdal)
library(raster)
library(rgeos)
library(sp)
library(tidyr)
library(dplyr)
library(osmdata)
library(mapview)
library(RANN)
library(ggplot2)
library(stargazer)
library(table1)
library(summarytools)
library(arsenal)
library(expss)
library(spatstat)
options(scipen=999)
options(tigris_class = "sf")

#Load Styles

mapTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 14,colour = "black"),
    plot.subtitle=element_text(face="italic"),
    plot.caption=element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),axis.title = element_blank(),
    axis.text = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=2)
  )
}

plotTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 14,colour = "black"),
    plot.subtitle = element_text(face="italic"),
    plot.caption = element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_line("grey80", size = 0.1),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=2),
    strip.background = element_rect(fill = "grey80", color = "white"),
    strip.text = element_text(size=12),
    axis.title = element_text(size=12),
    axis.text = element_text(size=10),
    plot.background = element_blank(),
    legend.background = element_blank(),
    legend.title = element_text(colour = "black", face = "italic"),
    legend.text = element_text(colour = "black", face = "italic"),
    strip.text.x = element_text(size = 14)
  )
}

# Load Quantile break functions

qBr <- function(df, variable, rnd) {
  if (missing(rnd)) {
    as.character(quantile(round(df[[variable]],0),
                          c(.01,.2,.4,.6,.8), na.rm=T))
  } else if (rnd == FALSE | rnd == F) {
    as.character(formatC(quantile(df[[variable]]), digits = 3),
                 c(.01,.2,.4,.6,.8), na.rm=T)
  }
}

q5 <- function(variable) {as.factor(ntile(variable, 5))}

# Load hexadecimal color palette

palette5 <- c("#25CB10", "#5AB60C", "#8FA108",   "#C48C04", "#FA7800")
palette6 <- c("#C48C04", "#FA7800","#8FA108",   "#5AB60C", "#25CB10")

# Load nn function

nn_function <- function(measureFrom,measureTo,k) { 
  measureFrom_Matrix <- as.matrix(measureFrom)
  measureTo_Matrix <- as.matrix(measureTo)
  nn <-   
    get.knnx(measureTo, measureFrom, k)$nn.dist
  output <-
    as.data.frame(nn) %>%
    rownames_to_column(var = "thisPoint") %>%
    gather(points, point_distance, V1:ncol(.)) %>%
    arrange(as.numeric(thisPoint)) %>%
    group_by(thisPoint) %>%
    summarize(pointDistance = mean(point_distance)) %>%
    arrange(as.numeric(thisPoint)) %>% 
    dplyr::select(-thisPoint) %>%
    pull()}
  
# Load census API key

census_api_key("91a259a2aaac3093a636d189040e0ff263fc823b", overwrite = TRUE)

# Load Assignment 2 student data

Miami_Houses <- 
  rbind(
    st_read("studentsData.geojson") %>%
    st_transform(st_crs('EPSG:6346')))
Miami_Houses <- st_set_crs(Miami_Houses, 6346)
Miami_Houses <- distinct(Miami_Houses,  .keep_all = TRUE)
```

```{r Loading Variables, results='hide', error=FALSE, message=FALSE, warning=FALSE}
# Load neighborhood boundaries

nhoodsmiami <- 
  rbind(
  st_read("https://opendata.arcgis.com/datasets/2f54a0cbd67046f2bd100fb735176e6c_0.geojson") %>%
  st_transform('EPSG:6346'))
nhoodsmiami <- st_set_crs(nhoodsmiami, 6346)

nhoodsmiamibeach <-
  rbind(st_read("neighborhoods/miamineighborhoods.shp")%>%
  st_transform('EPSG:6346'))
nhoodsmiamibeach <- st_set_crs(nhoodsmiamibeach,6346)

nhoodsmiami <- nhoodsmiami[2]
nhoodsmiamibeach <-nhoodsmiamibeach[1]%>%
 rename(LABEL = Name)
nhoodsmiami <-st_zm(nhoodsmiami, drop = TRUE, what = "ZM")
nhoodsmiamibeach <-st_zm(nhoodsmiamibeach, drop=TRUE, what = "ZM")
nhoods <- rbind(nhoodsmiami,nhoodsmiamibeach)%>%
  rename(neighborhood = LABEL)


# Create column with neighborhoods name

Miami_Houses <- st_join(Miami_Houses, nhoods, join = st_within)

# fix neighborhoods with limited entries

Miami_Houses$neighborhood2 <- ifelse(grepl("East Grove", Miami_Houses$neighborhood),Miami_Houses$neighborhood2<-"North Grove",
                                     ifelse(grepl("Bird Grove West", Miami_Houses$neighborhood), Miami_Houses$neighborhood2<-"Bird Grove East",
                                            ifelse(grepl("Bay Heights", Miami_Houses$neighborhood), Miami_Houses$neighborhood2<-"North Grove",
                                                   ifelse(grepl("Vizcaya", Miami_Houses$neighborhood), Miami_Houses$neighborhood2<-"North Grove",
                                                          ifelse(grepl("Fair Isle", Miami_Houses$neighborhood), Miami_Houses$neighborhood2<-"North Grove", Miami_Houses$neighborhood2<-Miami_Houses$neighborhood)))))


# Load demographic data from ACS

tracts18 <- 
  get_acs(geography = "tract", variables = c("B25026_001E","B02001_002E","B15001_050E",
                                             "B15001_009E","B19013_001E","B25058_001E",
                                             "B06012_002E"), 
          year=2018, state=12, county="Miami-Dade County", geometry=T, output="wide") %>%
  st_transform('EPSG:6346')%>%
  rename(TotalPop = B25026_001E, 
         Whites = B02001_002E,
         FemaleBachelors = B15001_050E, 
         MaleBachelors = B15001_009E,
         MedHHInc = B19013_001E, 
         MedRent = B25058_001E,
         TotalPoverty = B06012_002E) %>%
  dplyr::select(-NAME, -starts_with("B")) %>%
  mutate(pctWhite = ifelse(TotalPop > 0, Whites / TotalPop, 0),
         pctBachelors = ifelse(TotalPop > 0, ((FemaleBachelors + MaleBachelors) / TotalPop), 0),
         pctPoverty = ifelse(TotalPop > 0, TotalPoverty / TotalPop, 0),
         year = "2018") %>%
  dplyr::select(-Whites, -TotalPoverty) 

# select only tracts in Miami/Miami Beach

miami <- st_union(nhoods)
tracts.miami.intersect <- st_intersects(miami, tracts18)
tracts18miami <- tracts18[tracts.miami.intersect[[1]],]

# create columns with Census variables

Miami_Houses <- st_join(Miami_Houses,tracts18miami, join=st_within)

# Load Crime point data

miamicrime <- 
  rbind(
    st_read("MiamiCrime/09-27-10-03-2020-miamicrime.shp") %>% 
      st_transform(st_crs('EPSG:6346')))

# Create columns for nearest neighbor crime

st_c <- st_coordinates

Miami_Houses <-
  Miami_Houses %>% 
  mutate(
    crime_nn2 = nn_function(st_c(st_centroid(Miami_Houses)),
      st_c(st_centroid(miamicrime)), 2)) 
 

# create columns for house attributes

Miami_Houses$Pool <- ifelse(grepl("Pool", Miami_Houses$XF1), Miami_Houses$Pool<-"yes",
                            ifelse(grepl("Pool", Miami_Houses$XF2), Miami_Houses$Pool<-"yes",
                                          ifelse(grepl("Pool", Miami_Houses$XF3), Miami_Houses$Pool<-"yes",Miami_Houses$Pool<-"no")))

Miami_Houses$Patio <- ifelse(grepl("Patio", Miami_Houses$XF1), Miami_Houses$Patio<-"yes",
                            ifelse(grepl("Patio", Miami_Houses$XF2), Miami_Houses$Patio<-"yes",
                                   ifelse(grepl("Patio", Miami_Houses$XF3), Miami_Houses$Patio<-"yes",Miami_Houses$Patio<-"no")))

Miami_Houses$Carport <- ifelse(grepl("Carport", Miami_Houses$XF1), Miami_Houses$Carport<-"yes",
                            ifelse(grepl("Carport", Miami_Houses$XF2), Miami_Houses$Carport<-"yes",
                                   ifelse(grepl("Carport", Miami_Houses$XF3), Miami_Houses$Carport<-"yes",Miami_Houses$Carport<-"no")))

Miami_Houses$Whirlpool <- ifelse(grepl("Whirlpool", Miami_Houses$XF1), Miami_Houses$Whirlpool<-"yes",
                            ifelse(grepl("Whirlpool", Miami_Houses$XF2), Miami_Houses$Whirlpool<-"yes",
                                   ifelse(grepl("Whirlpool", Miami_Houses$XF3), Miami_Houses$Whirlpool<-"yes",Miami_Houses$Whirlpool<-"no")))

Miami_Houses$Dock <- ifelse(grepl("Dock", Miami_Houses$XF1), Miami_Houses$Dock<-"yes",
                            ifelse(grepl("Dock", Miami_Houses$XF2), Miami_Houses$Dock<-"yes",
                                   ifelse(grepl("Dock", Miami_Houses$XF3), Miami_Houses$Dock<-"yes",Miami_Houses$Dock<-"no")))

# load beach feature

miamibeach <- 
  rbind(
    st_read("https://opendata.arcgis.com/datasets/d0d6e6c9d47145a0b05d6621ef29d731_0.geojson") %>%
      st_transform('EPSG:6346'))
miamibeach <- st_set_crs(miamibeach, 6346)

# add column for distance to beach

miamibeach <- st_union(miamibeach)
Miami_Houses.centroids <-st_centroid(Miami_Houses)
Miami_Houses$beachDist <-st_distance(Miami_Houses.centroids, miamibeach)
Miami_Houses$beachDist <-as.numeric(Miami_Houses$beachDist)

# load water feature

miamiwater <-
  rbind(st_read("Water/miamiwater.shp")%>%
          st_transform('EPSG:6346'))
miamiwater <- st_set_crs(miamiwater,6346)

# add column for distance to water

miamiwater <- st_union(miamiwater)
Miami_Houses$waterDist <-st_distance(Miami_Houses.centroids, miamiwater)
Miami_Houses$waterDist <-as.numeric(Miami_Houses$waterDist)


# add parks feature

miami_municipalparks <- 
  rbind(
    st_read("https://opendata.arcgis.com/datasets/a585b193a4764760802f510f8c5b1452_0.geojson") %>%
      st_transform('EPSG:6346'))
miami_municipalparks <- st_set_crs(miami_municipalparks, 6346)

#add column for distance to park

miami_municipalparks <- st_union(miami_municipalparks)
Miami_Houses$ParksDist <-st_distance(Miami_Houses.centroids,miami_municipalparks)
Miami_Houses$ParksDist <-as.numeric(Miami_Houses$ParksDist)


# load Starbucks points

Starbucks <- st_read("Starbucks.csv")
Starbucks.sf <- st_as_sf(Starbucks, coords = c("Longitude", "Latitude"), crs = 4326, agr = "constant") %>% 
  st_transform(st_crs(Miami_Houses))

st_c <- st_coordinates

# add column for nearest neighbor Starbucks

Miami_Houses <-
  Miami_Houses %>% 
  mutate(
    Starbucks_nn1 = nn_function(st_c(st_centroid(Miami_Houses)), st_c(st_centroid(Starbucks.sf)), 1))

# load golf course feature

miamigolfcourses <- 
  rbind(
    st_read("https://opendata.arcgis.com/datasets/229eeac512b043f8bf5317ec8377f151_0.geojson") %>%
      st_transform('EPSG:6346'))
miamigolfcourses <- st_set_crs(miamigolfcourses, 6346)

# add column for distance to golf course

miamigolfcourses <- st_union(miamigolfcourses)
Miami_Houses$GolfCourseDist<-st_distance(Miami_Houses.centroids,miamigolfcourses)
Miami_Houses$GolfCourseDist <-as.numeric(Miami_Houses$GolfCourseDist)

# load metro stations

miami_metros <- st_read("MetroRailStations/Metrorail_Station.csv")
miami_metros.sf <- st_as_sf(miami_metros, coords = c("LON","LAT"), crs = 4326, agr = "constant") %>% 
  st_transform(st_crs(Miami_Houses))
miami_metros.sf <- st_set_crs(miami_metros.sf, 6346)
                               
st_c <- st_coordinates

Miami_Houses.centroids <-st_centroid(miami_metros.sf)

# add column for distance to metro 

Miami_Houses <-
  Miami_Houses %>% 
  mutate(
    Metros_nn1 = nn_function(st_c(st_centroid(Miami_Houses)), st_c(st_centroid(miami_metros.sf)), 1))

# load miami highways

miamiHighways <- 
  rbind(
    st_read("https://opendata.arcgis.com/datasets/6d31141fd24148f0b352f341ef38d161_0.geojson") %>%
      st_transform('EPSG:6346'))
miamiHighways <- st_set_crs(miamiHighways,6346)

miamiHighwaysbuffer.125 <- 
  rbind(
    st_union(st_buffer(miamiHighways, 201.168)) %>%
      st_sf() %>%
      mutate(Legend = "Unioned Buffer"))
miamiHighwaysbuffer.125<-miamiHighwaysbuffer.125%>%
  rename(buffer.125 = Legend)
miamiHighwaysbuffer.25 <- 
  rbind(
    st_union(st_buffer(miamiHighways, 402.336)) %>%
      st_sf() %>%
      mutate(Legend = "Unioned Buffer"))
miamiHighwaysbuffer.25<-miamiHighwaysbuffer.25%>%
  rename(buffer.25 = Legend)
miamiHighwaysbuffer.5 <- 
  rbind(
    st_union(st_buffer(miamiHighways, 804.672)) %>%
      st_sf() %>%
      mutate(Legend = "Unioned Buffer"))
miamiHighwaysbuffer.5<-miamiHighwaysbuffer.5%>%
  rename(buffer.5 = Legend)

Miami_Houses <- st_join(Miami_Houses, miamiHighwaysbuffer.125, join = st_within)
Miami_Houses <- st_join(Miami_Houses, miamiHighwaysbuffer.25, join = st_within)
Miami_Houses <- st_join(Miami_Houses, miamiHighwaysbuffer.5, join = st_within)

# add column for distance to highway

Miami_Houses$Highwaydist <- ifelse(grepl("Unioned Buffer", Miami_Houses$buffer.125), Miami_Houses$Highwaydist<-".125",
                            ifelse(grepl("Unioned Buffer", Miami_Houses$buffer.25), Miami_Houses$Highwaydist<-".25",
                                   ifelse(grepl("Unioned Buffer", Miami_Houses$buffer.5), Miami_Houses$Highwaydist<-".5",Miami_Houses$Highwaydist<-"over .5")))


# load middle school districts

miami.middleschools <- 
  rbind(
    st_read("https://opendata.arcgis.com/datasets/dd2719ff6105463187197165a9c8dd5c_0.geojson") %>%
      st_transform('EPSG:6346'))
miami.middleschools <- st_set_crs(miami.middleschools,6346)
miami.middleschools <- miami.middleschools[,3]%>%rename(midschool = NAME)

# add column for middle school district

Miami_Houses <- st_join(Miami_Houses, miami.middleschools, join = st_within)

# load school districts

miami.schooldistricts <- 
  rbind(
    st_read("https://opendata.arcgis.com/datasets/bc16a5ebcdcd4f3e83b55c5d697a0317_0.geojson") %>%
      st_transform('EPSG:6346'))
miami.schooldistricts <- st_set_crs(miami.schooldistricts,6346)
miami.schooldistricts <- miami.schooldistricts[,2]%>%
  rename(schooldist = ID)

# add column for school districts

Miami_Houses <- st_join(Miami_Houses, miami.schooldistricts, join = st_within)

# load daycares

miami.daycares <- 
  rbind(
    st_read("https://opendata.arcgis.com/datasets/3ea3c3aa067549ff8f8a8ab80a3cbcbb_0.geojson") %>%
      st_transform('EPSG:6346'))
miami.daycares <- st_set_crs(miami.daycares,6346)
st_c <- st_coordinates

# add columns for nearest neighbor daycares

Miami_Houses <-
  Miami_Houses %>% 
  mutate(
    daycare_nn2 = nn_function(st_c(st_centroid(Miami_Houses)), st_c(st_centroid(miami.daycares)), 2))


# load colleges

miami.colleges <- 
  rbind(
    st_read("https://opendata.arcgis.com/datasets/7db056c406b943dc8f3f377b99d77588_0.geojson") %>%
      st_transform('EPSG:6346'))
miami.colleges <- st_set_crs(miami.colleges,6346)
st_c <- st_coordinates

# add columns for nearest neighbor colleges

Miami_Houses <-
  Miami_Houses %>% 
  mutate(
    colleges_nn3 = nn_function(st_c(st_centroid(Miami_Houses)), st_c(st_centroid(miami.colleges)), 3))


# load contaminated sites

miami.contamination <- 
  rbind(
    st_read("https://opendata.arcgis.com/datasets/43750f842b1e451aa0347a2ca34a61d7_0.geojson") %>%
      st_transform('EPSG:6346'))
miami.contamination <- st_set_crs(miami.contamination,6346)
st_c <- st_coordinates

# add columns for contaminated sites nearest neighbor

Miami_Houses <-
  Miami_Houses %>% 
  mutate(
    contamination_nn3 = nn_function(st_c(st_centroid(Miami_Houses)), st_c(st_centroid(miami.contamination)), 3))


#  load private schools

miami.pschool <- 
  rbind(
    st_read("https://opendata.arcgis.com/datasets/7fecb87ea1b1494eb2beb13906465de9_0.geojson") %>%
      st_transform('EPSG:6346'))
miami.pschool <- st_set_crs(miami.pschool,6346)
st_c <- st_coordinates

# add columns for private schools nearest neighbor

Miami_Houses <-
  Miami_Houses %>% 
  mutate(
    pschool_nn3 = nn_function(st_c(st_centroid(Miami_Houses)), st_c(st_centroid(miami.pschool)), 3))

# load hospitals

miami.hospitals <- 
  rbind(
    st_read("https://opendata.arcgis.com/datasets/0067a0e8b40644f980afa23ad34c32c4_0.geojson") %>%
      st_transform('EPSG:6346'))
miami.hospitals <- st_set_crs(miami.hospitals,6346)
st_c <- st_coordinates

# add columns for hospital nearest neighbor

Miami_Houses <-
  Miami_Houses %>% 
  mutate(
    hospitals_nn3 = nn_function(st_c(st_centroid(Miami_Houses)), st_c(st_centroid(miami.hospitals)), 3))


#load marinas

miami.marinas <- 
  rbind(
    st_read("https://opendata.arcgis.com/datasets/f65dec3bacb341f094dd5109e93c4247_0.geojson") %>%
      st_transform('EPSG:6346'))
miami.marinas <- st_set_crs(miami.marinas,6346)
st_c <- st_coordinates

# add columns for marinas nearest neighbor

Miami_Houses <-
  Miami_Houses %>% 
  mutate(
    marinas_nn2 = nn_function(st_c(st_centroid(Miami_Houses)), st_c(st_centroid(miami.marinas)), 2))

# add column for spatial lag of Sq ft

Miami_Houses.centroids<-st_centroid(Miami_Houses)
coords.test <- st_centroid(st_geometry(Miami_Houses), of_largest_polygon=TRUE)
coords <-  st_coordinates(Miami_Houses.centroids)
neighborList <- knn2nb(knearneigh(coords.test, 5))
spatialWeights <- nb2listw(neighborList, style="W")
Miami_Houses$lagSQ <- lag.listw(spatialWeights, Miami_Houses$ActualSqFt)

# add column for spatial lag of Lot Size

Miami_Houses.centroids<-st_centroid(Miami_Houses)
coords.test <- st_centroid(st_geometry(Miami_Houses), of_largest_polygon=TRUE)
coords <-  st_coordinates(Miami_Houses.centroids)
neighborList <- knn2nb(knearneigh(coords.test, 5))
spatialWeights <- nb2listw(neighborList, style="W")
Miami_Houses$lagLot <- lag.listw(spatialWeights, Miami_Houses$LotSize)

# create training data

Miami_Training <- subset(Miami_Houses, toPredict %in% 0)

#create test data

`%nin%` = Negate(`%in%`)
Miami_Test <- subset(Miami_Houses,toPredict %nin% 0)

# finding average sale price of 5 nearest neighbors Test set

Miami_TestPPP <-as.ppp(st_centroid(Miami_Test))
Miami_TrainingPPP<-as.ppp(st_centroid(Miami_Training))

Miami_Test$nnHouse1 <- nncross(Miami_TestPPP, Miami_TrainingPPP, what="which",k=1)
Miami_Test$nnHouse2 <- nncross(Miami_TestPPP, Miami_TrainingPPP, what="which",k=2)
Miami_Test$nnHouse3 <- nncross(Miami_TestPPP, Miami_TrainingPPP, what="which",k=3)
Miami_Test$nnHouse4 <- nncross(Miami_TestPPP, Miami_TrainingPPP, what="which",k=4)
Miami_Test$nnHouse5 <- nncross(Miami_TestPPP, Miami_TrainingPPP, what="which",k=5)
Miami_Training$ID <- 1:1819

price <- function(data) {
  price1<-Miami_Training$SalePrice[Miami_Training$ID==data]
  return(price1)
}

Miami_Test$price1 <- lapply(Miami_Test$nnHouse1,price)
Miami_Test$price2 <- lapply(Miami_Test$nnHouse2,price)
Miami_Test$price3 <- lapply(Miami_Test$nnHouse3,price)
Miami_Test$price4 <- lapply(Miami_Test$nnHouse4,price)
Miami_Test$price5 <- lapply(Miami_Test$nnHouse5,price)

Miami_Test$price1 <- as.numeric(Miami_Test$price1)
Miami_Test$price2 <- as.numeric(Miami_Test$price2)
Miami_Test$price3 <- as.numeric(Miami_Test$price3)
Miami_Test$price4 <- as.numeric(Miami_Test$price4)
Miami_Test$price5 <- as.numeric(Miami_Test$price5)

Miami_Test$SalePriceAvg <- (Miami_Test$price1+Miami_Test$price2+Miami_Test$price3+Miami_Test$price4+Miami_Test$price5)/5

# finding average sale price of 5 nearest neighbors Training Set

Miami_Training$nnHouse1 <- nncross(Miami_TrainingPPP, Miami_TrainingPPP, what="which",k=2)
Miami_Training$nnHouse2 <- nncross(Miami_TrainingPPP, Miami_TrainingPPP, what="which",k=3)
Miami_Training$nnHouse3 <- nncross(Miami_TrainingPPP, Miami_TrainingPPP, what="which",k=4)
Miami_Training$nnHouse4 <- nncross(Miami_TrainingPPP, Miami_TrainingPPP, what="which",k=5)
Miami_Training$nnHouse5 <- nncross(Miami_TrainingPPP, Miami_TrainingPPP, what="which",k=6)

price <- function(data) {
  price1<-Miami_Training$SalePrice[Miami_Training$ID==data]
  return(price1)
}

Miami_Training$price1 <- lapply(Miami_Training$nnHouse1,price)
Miami_Training$price2 <- lapply(Miami_Training$nnHouse2,price)
Miami_Training$price3 <- lapply(Miami_Training$nnHouse3,price)
Miami_Training$price4 <- lapply(Miami_Training$nnHouse4,price)
Miami_Training$price5 <- lapply(Miami_Training$nnHouse5,price)

Miami_Training$price1 <- as.numeric(Miami_Training$price1)
Miami_Training$price2 <- as.numeric(Miami_Training$price2)
Miami_Training$price3 <- as.numeric(Miami_Training$price3)
Miami_Training$price4 <- as.numeric(Miami_Training$price4)
Miami_Training$price5 <- as.numeric(Miami_Training$price5)

Miami_Training$SalePriceAvg <- (Miami_Training$price1+Miami_Training$price2+Miami_Training$price3+Miami_Training$price4+Miami_Training$price5)/5

Miami_TrainingGeom <- Miami_Training
```

```{r Miami Sale Price distribution, results='hide', error=FALSE, message=FALSE, warning=FALSE,fig.asp=.75}

# Plot distribution of sale prices over neighborhoods

ggplot() +
  geom_sf(data = nhoods, fill = "grey40") +
  geom_sf(data = st_centroid(Miami_Training), aes(colour = q5(SalePrice)), 
          show.legend = "point", size = .75) +
  scale_colour_manual(values = palette5,
                      labels=qBr(Miami_Training,"SalePrice"),
                      name="Quintile\nBreaks") +
  labs(title="Sale Price (Dollars)",
  subtitle="Miami, Florida", 
       caption= "Figure 1.1.1") +
  mapTheme()

```

# 1.2 Data 

## Features

While our final model only includes the features of neighborhood, zoning class, average sale price of the five nearest homes, and dock prescence, we began by collecting as many features as we could find. We found data for our features from online open data sources, including the [Miami-Dade County Open Data Hub](https://gis-mdc.opendata.arcgis.com/). We also engineered features from within the dataset we were given. Our original features included everything from distance to the beach to middle school district. After thorough analysis, we narrowed our features down to the final four included in the model. Our features were split between internal characteristics, amenities/public services, and spatial structures. 

```{r Create Training data, results='hide', error=FALSE, message=FALSE, warning=FALSE,fig.asp=0.75}

# internal characteristics

Miami_Training_internal<-Miami_Training[,c("Bed","LotSize",
                                          "Bath","YearBuilt","Stories","ActualSqFt","Pool",  
                                          "Patio", "Carport","Whirlpool",            
                                          "Dock")]

# spatial characteristics

Miami_Training_spatial<-Miami_Training[,c("Property.City","Zoning", "crime_nn2", "neighborhood" , "contamination_nn3",  "MedHHInc", "MedRent","pctWhite", "pctBachelors", "pctPoverty",  "lagLot" , "lagSQ", "SalePriceAvg")]

# amenities characteristics

Miami_Training_amenities<-Miami_Training[,c( "beachDist", "waterDist" ,"GolfCourseDist", "Metros_nn1",  "Highwaydist" ,  "midschool","schooldist", "daycare_nn2" , 
                                           "colleges_nn3",        
                                           "pschool_nn3", "hospitals_nn3",        
                                           "marinas_nn2" )]


```

### Internal Characteristics

The following features of internal characteristics were created:

Variable Name | Feature | Units | Type
------------- | ------------- | -------------| -------------
ActualSqFt| Actual Square Feet | Square Feet | Numerical
Bath|Number of Baths | - | Categorical 
Bed  |Number of Beds | - | Categorical 
Carport | Car Port | - | Categorical 
Dock | Dock | - | Categorical 
LotSize | Lot Size | Square Feet | Numerical
Patio | Patio | - |Categorical 
Pool | Pool | - | Categorical 
Whirlpool | Whirlpool | - | Categorical 
YearBuilt | Year Built | Years | Numerical 

``````{r feature summary statistics, results='asis',fig.width=20, fig.height=8}

# summary statistics of internal characteristics

stargazer(
  as.data.frame(Miami_Training_internal[,c("LotSize","YearBuilt","ActualSqFt")]), type = "html",
  summary.stat = c("min", "p25", "median", "p75", "max", "median", "sd"),
  header = FALSE,
  title = "Internal Characteristics Summary Statistics",
  notes = "Table 1.2.1 ",
                notes.align = "l",
                float = TRUE,
                table.placement = "H")

```

### Amenities and Public Services

The following features of amenities and public services were created:

Variable Name | Feature | Units | Type
------------- | ------------- | -------------| -------------
beachDist| Dist ance to beach | Meters | Numerical
colleges_nn3  |Average distance to three nearest colleges| Meters| Numerical 
daycare_nn2| Average distance to two nearest daycares | Meters | Numerical 
GolfCourseDist | Distance to golf course | Meters | Numerical 
Highwaydist | Distance to major highway | Meters | Categorical 
hospitals_nn3 | Average distance to three nearest hospitals | Meters | Numerical
marinas_nn2 | Average distance to two nearest marinas | Meters |Numerical 
Metros_nn1 | Distance to nearest metro station | Meters | Numerical 
midschool | Middle School | - | Categorical 
pschool_nn3 | Average distance to three nearest private schools | Meters | Numerical
schooldist | School District | - | Categorical
waterdist | Distance to water | Meters | Numerical

``````{r feature summary statistics2, results='asis',fig.width=20, fig.height=8}

# summary statistics of amenities

stargazer(
  as.data.frame(Miami_Training_amenities[,c("beachDist", "waterDist" ,"GolfCourseDist", "Metros_nn1", 
                                           "Highwaydist" ,  "midschool","schooldist" , "daycare_nn2" , 
                                           "colleges_nn3",        
                                           "pschool_nn3", "hospitals_nn3",        
                                           "marinas_nn2" )]), type = "html",
  summary.stat = c("min", "p25", "median", "p75", "max", "median", "sd"),
  header = FALSE,
  title = "Amenities and Public Services Summary Statistics",
  notes = "Table 1.2.2 ",
                notes.align = "l",
                float = TRUE,
                table.placement = "H")
```

### Saptial Structures

The following features of spatial structures were created:

Variable Name | Feature | Units | Type
------------- | ------------- | -------------| -------------
contamination_nn3 |Average distance to three nearest contamination sites| Meters| Numerical
crime_nn2| Average distance to two nearest crime incidents | Meters | Numerical
lagLot  |Spatial Lag Lot Size| Square Feet | Numerical 
lagSQ | Spatia Lag Actual Square Feet | Square Feet | Numerical 
MedHHInc | Median Household Income of census tract | Dollars | Numerical 
MedRent | Median Rent of census tract | Dollars | Numerical 
pctBachelors | Percent of persons with Bachelor's degree by census tract | Percent | Numerical
pctPoverty | Percent of persons below poverty line by census tract | Percent | Numerical 
pctWhite | Percent of persons identifying as white by census tract | Percent | Numerical 
SalePriceAvg | Average sale price of five nearest homes |Dollars  | Numerical
Zoning | Zoning Class | - | Categorical


``````{r feature summary statistics3, results='asis',fig.width=20, fig.height=8}

# summary statisitcs of spatial structures

stargazer(
  as.data.frame(Miami_Training_spatial[,c("Property.City","Zoning", "crime_nn2", "neighborhood" , "contamination_nn3",  "MedHHInc", "MedRent","pctWhite", "pctBachelors", "pctPoverty",  "lagLot" , "lagSQ", "SalePriceAvg")]), type = "html",
  summary.stat = c("min", "p25", "median", "p75", "max", "median", "sd"),
  header = FALSE,
  title = "Amenities and Public Services Summary Statistics",
  notes = "Table 1.2.3 ",
                notes.align = "l",
                float = TRUE,
                table.placement = "H")
```

## Data Exploration

We created a correlation matrix to examine the correlations between our features. A good model will avoid including features that are strongly correlated with one another, because if both correlated features are put into the regression, one will be insignificant. We found that while lagLot and lagSQ were the only two features strongly correlated with each other (r > .8), many other features were moderately correlated. We carefully examined these correlations, as well as each feature's correlation with sale price to decide which features to use in our model.

```{r Correlation Matrix, results='hide', error=FALSE, message=FALSE, warning=FALSE,fig.asp=1}

# select features for correlation matrix

Miami_Training_features<-Miami_Training[,c("SalePrice","LotSize",
                                          "YearBuilt","ActualSqFt","Pool",                 
                                          "Patio", "Carport","Whirlpool",            
                                          "Dock","Zoning", "crime_nn2", "neighborhood" , "contamination_nn3",  "MedHHInc", "MedRent","pctWhite", "pctBachelors", "pctPoverty",  "lagLot" , "lagSQ", "SalePriceAvg","beachDist", "waterDist" ,"GolfCourseDist", "Metros_nn1", 
"Highwaydist" ,  "midschool","schooldist" , "daycare_nn2" , 
                                           "colleges_nn3",        
                                           "pschool_nn3", "hospitals_nn3",        
                                           "marinas_nn2")]

# select numeric variables

numericVars <- 
  select_if(Miami_Training_features, is.numeric) %>% na.omit()
numericVars <-st_drop_geometry(numericVars)

cor(numericVars)

# correlation matrix of variables

Figure1.1 <- ggcorrplot(
  round(cor(numericVars), 1), 
  p.mat = cor_pmat(numericVars),
  colors = c("#FA7800","white","#25CB10"),
  type="lower",
  insig = "blank") +  
  labs(title = "Correlation across numeric variables", caption = "Figure 1.2.1") 


plot(Figure1.1)
```

The feature with strongest statistically significant correlation to sale price was the actual square foot of the home with a correlation coefficient of .88. Other features with strong correlations to sale price included SalePriceAvg (r=.76),LotSize (r=.71), and lagSQ (r=.65). Some of the next strongest positive correlations included daycare_nn2 (r=.45) and MedHHInc (r=.41). These relationships are explored through the following scatter plots. 

```{r Correlation scatter plots, results='hide', error=FALSE, message=FALSE, warning=FALSE,fig.asp=.5}

# feature scatter plotss

st_drop_geometry(Miami_Training) %>% 
  dplyr::select(SalePrice, ActualSqFt, SalePriceAvg, daycare_nn2, MedHHInc) %>%
  filter(SalePrice <= 1000000) %>%
  gather(Variable, Value, -SalePrice) %>% 
   ggplot(aes(Value, SalePrice)) +
     geom_point(size = .5) + geom_smooth(method = "lm", se=F, colour = "#FA7800") +
     facet_wrap(~Variable, ncol = 3, scales = "free") +
     labs(title = "Price as a function of continuous variables", x= "", y="Sale Price",caption= "Figure 1.2.2") +
     plotTheme()

```

In all four plots, the regression line slopes upward, showing that as the variables increase, so does sale price. While scatter plots are effective for exploring numerical features, many of our features were categorical. Instead, we examined the relationship between these features and the sale price using bar charts.

```{r Correlation bar charts, results='hide', error=FALSE, message=FALSE, warning=FALSE,fig.asp=1.5,fig.width=10}

#correlation bar charts

st_drop_geometry(Miami_TrainingGeom) %>% 
  dplyr::select(SalePrice, Zoning, midschool, Dock, neighborhood2) %>%
  filter(SalePrice <= 1000000) %>%
  gather(Variable, Value, -SalePrice) %>% 
   ggplot(aes(Value, SalePrice)) +
     geom_bar(position = "dodge", stat = "summary", fun.y = "mean") +
     facet_wrap(~Variable, ncol = 1, scales = "free") +
     labs(title = "Price as a function of categorical variables", y = "Mean Sale Price", x=" ", caption="Figure 1.2.3") +
     theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
     plotTheme()

```

Maps can also be used to examine the relationship between features and sale price. By comparing maps with each other, we were able to investigate spatial patterns that informed our model. 

```{r average sale price map, results='hide', error=FALSE, message=FALSE, warning=FALSE,fig.asp=.75}


# Plot zoning

ggplot() +
  geom_sf(data = nhoods, fill = "grey40") +
  geom_sf(data = st_centroid(Miami_TrainingGeom), aes(colour = Zoning), 
          show.legend = "point", size = .75) +
  labs(title="Zoning Classes",
  subtitle="Miami, Florida", 
       caption= "Figure 1.2.4") +
  mapTheme()

#plot Dock

ggplot() +
  geom_sf(data = nhoods, fill = "grey40") +
  geom_sf(data = st_centroid(Miami_TrainingGeom), aes(colour = Dock), 
          show.legend = "point", size = .75) +
  labs(title="Properties with Docks",
  subtitle="Miami, Florida", 
       caption= "Figure 1.2.5") +
  mapTheme()

# Plot distribution of sale prices over neighborhoods

ggplot() +
  geom_sf(data = nhoods, fill = "grey40") +
  geom_sf(data = st_centroid(Miami_TrainingGeom), aes(colour = q5(daycare_nn2)), 
          show.legend = "point", size = .75) +
  scale_colour_manual(values = palette5,
                      labels=qBr(Miami_TrainingGeom,"daycare_nn2"),
                      name="Quintile\nBreaks") +
  labs(title="Average distance (m) to two nearest daycare center",
  subtitle="Miami, Florida", 
       caption= "Figure 1.2.6") +
  mapTheme()

# Plot distribution of sale prices over neighborhoods

ggplot() +
  geom_sf(data = nhoods, fill = "grey40") +
  geom_sf(data = st_centroid(Miami_TrainingGeom), aes(colour = q5(SalePriceAvg)), 
          show.legend = "point", size = .75) +
  scale_colour_manual(values = palette5,
                      labels=qBr(Miami_TrainingGeom,"SalePriceAvg"),
                      name="Quintile\nBreaks") +
  labs(title="Average Sale Price of Five Nearest Neighbors",
  subtitle="Miami, Florida", 
       caption= "Figure 1.2.7") +
  mapTheme()
```

Figure 1.2.5 shows that unsurprisingly, homes with docks are clustered near the shore, and are especially prevalent in Miami Beach. Figures 1.2.4 and 1.2.6 show an interesting pattern between average distance to the nearest two daycare centers and zoning. It appears that daycare centers are less prevalent in areas that are in the zoning class 0100- single family- general. These are the types of patterns we considered when creating our model.

# 1.3 Methods

After our initial data exploration, we began the hard work of fine-tuning our price prediction model. We evaluated the predictive power of different combinations of variables by dividing the dataset into a "training" set and a "test" set that allowed us to test how our model might perform on a real dataset. 60% of houses with known sale values were grouped randomly into the training set, and 40% were grouped into the test set. To find the most accurate model, we created a list of every single combination of features and used a function to test every combination to determine which set produced the lowest mean absolute error (MAE), while using k-fold cross validation. After selecting a model, we analyzed how good our model was at predicting across neighborhoods and within neighborhoods and looked for clustering of errors.

# 1.4 Results

## Ordinary Least Squares Regression (OLS)

The output of a regression model using training set data and including our final variable selection of Neighborhood, nearby Sale Price Average, Zoning, and presence of a Dock is summarized below.

``````{r OLS, results='asis',error=FALSE, message=FALSE, warning=FALSE,fig.width=20, fig.height=8}


# create training and test set

inTrain <- createDataPartition(
  y = paste(Miami_TrainingGeom$neighborhood2), 
  p = .60, list = FALSE)
Miami_training_new <- Miami_TrainingGeom[inTrain,] 
Miami_test_new <- Miami_TrainingGeom[-inTrain,]  

# regression model

reg.training <- lm(SalePrice ~ ., data = st_drop_geometry(Miami_Training) %>% 
                      dplyr::select(SalePrice,neighborhood2,SalePriceAvg,Zoning,Dock))

# regression summary

reg.summary <- coef(summary(reg.training))
kable(reg.summary,format = "html", caption = "Regression Results Summary") %>% 
  kable_styling()

#create output table of regression summary

first_column <- c("Observations","R2", "Adjusted R2","Residual Std. Error","F Statistic")
second_column <- c("1,756","0.767","0.755","951,616.300(df = 1665)","60.970*** (df = 90; 1665) (p = 0.000)")

regression_stats <- data.frame(first_column, second_column)

kable(regression_stats,format = "html", col.names = c('', '')) %>% 
  kable_styling() %>%
  footnote(general_title = "\n",
           general = "Table 1.4.1")


```

The regression analysis summary shows that while the neighborhood and zoning class categories had varying degrees of significance, the presence of a dock and nearby sale price average were highly statistically significant. Our model has an R-squared value of 0.767 meaning that around three-fourths of the variance in sale price can be explained by our features.

## Prediction

Using the same regression variables described above, we applied our model to the houses in our test set in order to create predicted sale prices. In order to evaluate the effectiveness of the model, we started by calculating the average difference between the predicted prices and actual prices (Sale Price Error), the absolute value of those differences (Sale Price Absolute Error), and the percentage error of the absolute differences (Average Percentage Error). The output shown below shows the mean absolute error (MAE) and mean absolute percentage error (MAPE) of the test set.

``````{r MAE and MAPE, results='asis',error=FALSE, message=FALSE, warning=FALSE,fig.width=20, fig.height=2}

# single test set

Miami_test_new <-
  Miami_test_new %>%
  mutate(SalePrice.Predict = predict(reg.training, Miami_test_new),
         SalePrice.Error = SalePrice.Predict - SalePrice,
         SalePrice.AbsError = abs(SalePrice.Predict - SalePrice),
         SalePrice.APE = (abs(SalePrice.Predict - SalePrice)) / SalePrice.Predict)%>%
  filter(SalePrice < 5000000)
 
AbsError <- mean(Miami_test_new$SalePrice.AbsError, na.rm = T)
APE <- mean(Miami_test_new$SalePrice.APE, na.rm = T) 

#create a table of single test output

first_column <- c("Mean Absolute Error","Mean Absolute Percent Error")
second_column <- c(AbsError,APE)

Single_test <- data.frame(first_column, second_column)

kable(Single_test,format = "html", caption = "Prediction Results of Single Test", col.names = c('', '')) %>%
  kable_styling() %>%
  footnote(general_title = "\n",
           general = "Table 1.4.2")


```

The test set was further analyzed through a K-Fold Cross Validation technique. Instead of calculating the MAE and MAPE based on the results of one single randomly selected test set, this approach allows the process to be repeated 100 times and then averaged together to achieve a higher degree of accuracy and to judge generalizability. Our MAE was much smaller when performing only one test as compared with when using k-fold cross validation, which indicates that our model is more accurate than generalizable. The histogram below shows that for several runs, our model had very large MAE. This means that there must be a feature appearing in some of the random samples that our model is not accurately accounting for.  

```{r Cross Validation,echo=FALSE, error=FALSE, message=FALSE, warning=FALSE,fig.asp=.5}

#Cross validate the data using a 100 fold method

fitControl <- trainControl(method = "cv", number = 100)
set.seed(825)

reg.cv <- 
  train(SalePrice ~ ., data = st_drop_geometry(Miami_Training) %>% 
          dplyr::select(SalePrice,
                        neighborhood2,
                         SalePriceAvg,
                          Dock,
                          Zoning), 
        method = "lm", trControl = fitControl, na.action = na.pass)

#create a table of kfold output

first_column <- c("RMSE","R2","MAE")
second_column <- c(mean(reg.cv$resample[,1]),mean(reg.cv$resample[,2]),mean(reg.cv$resample[,3]))

kfold_results <- data.frame(first_column, second_column)

kable(kfold_results,format = "html", caption = "K-Fold Cross Validation Test", col.names = c('', '')) %>% 
  kable_styling() %>%
  footnote(general_title = "\n",
           general = "Table 1.4.3")

#Histogram of MAEs

MAEs <- data.frame("MAE"=reg.cv$resample[,3],"number"=1:100)
ggplot(data=MAEs, aes(MAEs$MAE)) + 
  geom_histogram(breaks=seq(0, 1500000, by=10000), 
                 fill="#25CB10" 
                 ) + 
  labs(title="Distribution of MAE", x="Mean Absolute Error", y="Count",  subtitle="k-fold cross validation; k=100", 
       caption= "Figure 1.4.1")+
  plotTheme()

```

```{r Test Regression,echo=FALSE, error=FALSE, message=FALSE, warning=FALSE,fig.asp=.5}

#Rename regression to "Baseline Regression"

Miami_test_new <-
  Miami_test_new %>%
  mutate(Regression = "Baseline Regression",
         SalePrice.Predict = predict(reg.training, Miami_test_new),
         SalePrice.Error = SalePrice.Predict - SalePrice,
         SalePrice.AbsError = abs(SalePrice.Predict - SalePrice),
         SalePrice.APE = (abs(SalePrice.Predict - SalePrice)) / SalePrice.Predict)%>%
  filter(SalePrice < 5000000) 
```

## Spatial Autocorrelation Checks

Plotting the prices predicted by the model against actual observed prices allowed us to diagnose where the model seemed to be predicting accurately and where it was not. Our predicted line (Green) starts above the perfect prediction line (Orange), suggesting that we are over-predicting home values for very low priced homes. This relationship soon reverses however, and as the graph shows, our model appears to increasingly under-predict the price of homes as the true sale price increases. 

```{r Plot predicted prices,echo=FALSE, error=FALSE, message=FALSE, warning=FALSE,fig.asp=.5}

#Plot prediction prices as function of observed prices

Miami_test_new %>%
  dplyr::select(SalePrice.Predict, SalePrice, Regression) %>%
  ggplot(aes(SalePrice, SalePrice.Predict)) +
  geom_point() +
  stat_smooth(aes(SalePrice, SalePrice), 
              method = "lm", se = FALSE, size = 1, colour="#FA7400") + 
  stat_smooth(aes(SalePrice.Predict, SalePrice), 
              method = "lm", se = FALSE, size = 1, colour="#25CB10") +
  facet_wrap(~Regression) +
  labs(title="Predicted sale price as a function of observed price",
       subtitle="Orange line represents a perfect prediction; Green line represents prediction", x= "Actual Sale Price",y="Predicted Sale Price",caption= "Figure 1.4.2") +
  plotTheme() + theme(plot.title = element_text(size = 18, colour = "black")) 

```

One of the metrics we evaluated for the test set was the absolute error of the predicted sale prices, which is calculated as the absolute value of the difference between our predicted sale price and the actual sale price. In the plot below, higher errors are represented by orange dots, while lower errors are represented by green dots. The map seems to suggests that our errors have spatial autocorrelation, which is to say that the instances of high errors seem to be clustered in the Southwest, along the coast, and within the Miami Beach peninsula. Based on the previous plot which suggests our model produces most error at higher home values, we hypothesize that the clustering visible on the map is at least partially driven by average sale price. The areas along the coast, within Miami Beach, and in the Southwest of the city limits (especially within a more upscale neighborhood called Southwest Coconut Grove) are also locations with especially high home values. Conversely, the areas with lowest Absolute Errors appear to be areas with lower home values. 

```{r residuals map,echo=FALSE, error=FALSE, message=FALSE, warning=FALSE,fig.asp=.75}

# Plot distribution of absolute sale price errors of test set over neighborhoods

ggplot() +
  geom_sf(data = nhoods, fill = "grey40") +
  geom_sf(data = st_centroid(Miami_test_new), aes(colour = q5(SalePrice.AbsError)), 
          show.legend = "point", size = .75) +
  scale_colour_manual(values = palette5,
                      labels=qBr(Miami_test_new,"SalePrice.AbsError"),
                      name="Quintile\nBreaks") +
  labs(title="Absolute sale price errors on the test set",
  subtitle="Miami, Florida", 
       caption= "Figure 1.4.3") +
  mapTheme()




```

In order to further analyze the question of whether errors in our model were spatially clustered, we looked at the spatial lag of sale price and sale price error. The plots below show sale price as a function of the spatial lag of price and error as a function of the spatial lag of error. Figure 1.4.4 shows that as price increases, so does the price of nearby houses. This is evidence of the clustering of home prices. Figure 1.4.5 shows that the error in our model is also clustered. The fact that the model's error is spatially autocorrelated indicates that important features are missing from the model.

```{r Spatial Lag,echo=FALSE, error=FALSE, message=FALSE, warning=FALSE,fig.asp=.5}

#Calculate and plot spatial lag of the training set

Miami_Training.centroids<-st_centroid(Miami_Training)
coords <- st_centroid(st_geometry(Miami_Training), of_largest_polygon=TRUE)
neighborList <- knn2nb(knearneigh(coords, 5))
spatialWeights <- nb2listw(neighborList, style="W")
Miami_Training$lagPrice <- lag.listw(spatialWeights, Miami_Training$SalePrice)

```

```{r Spatial Lag2,echo=FALSE, error=FALSE, message=FALSE, warning=FALSE,fig.asp=.75}

#Calculate spatial lag of the training set

Miami_Training.centroids<-st_centroid(Miami_Training)
coords <- st_centroid(st_geometry(Miami_Training), of_largest_polygon=TRUE)
neighborList <- knn2nb(knearneigh(coords, 5))
spatialWeights <- nb2listw(neighborList, style="W")
Miami_Training$lagPrice <- lag.listw(spatialWeights, Miami_Training$SalePrice)

#Calculate spatial lag of sale price error for the test set

coords.test <- st_centroid(st_geometry(Miami_test_new), of_largest_polygon=TRUE)
neighborList.test <- knn2nb(knearneigh(coords.test, 5))
spatialWeights.test <- nb2listw(neighborList.test, style="W")

Miami_test_plot <- Miami_test_new
Miami_test_plot[is.na(Miami_test_plot)] <- 0
Miami_test_plot$lagPriceError <- lag.listw(spatialWeights.test, Miami_test_plot$SalePrice.Error)
Miami_test_plot$lagPrice <- lag.listw(spatialWeights.test, Miami_test_plot$SalePrice)

# create plot of error as a function of spatial lage of error and price as a function of the spatial lag of price

plot2<-ggplot(data=Miami_test_plot, aes(lagPriceError,SalePrice.Error)) +
  geom_point()+
  geom_smooth(method=lm, se=FALSE)+
  labs(title="Error as a func. of the spatial lag of error", x="Spatial lag of errors (mean error of 5 nearest neighbors)", y="Sale Price Error",  
       caption= "Figure 1.4.5")+
  plotTheme()+
  theme(plot.title = element_text(size=10),axis.title = element_text(siz=10))

plot1 <-ggplot(data=Miami_test_plot, aes(lagPrice,SalePrice)) +
  geom_point()+
  geom_smooth(method=lm, se=FALSE)+
  labs(title="Price as a func. of the spatial lag of price", x="Spatial lag of price (mean of 5 nearest neighbors)", y="Sale Price",  
       caption= "Figure 1.4.4")+
  plotTheme()+
  theme(plot.title = element_text(size=10), axis.title = element_text(siz=10))

grid.arrange(plot1,plot2, ncol=2)

```

We also examined spatial autocorrelation by looking at Moran's I, which can be interpreted as a measure of spatial autocorrelation. The randomly permuted Moran's I's are plotted in the histogram in grey, while the actual Moran's I of our data is represented by the orange line on the graph. The fact that this line is located at a value that is positive and significantly different than zero, implies that there is indeed more spatial autocorrelation to the clustering of our errors than would be explained by random chance. This finding validates our suspicion when looking at the map of absolute errors that our model is failing to fully account for some real-world spatial variable or several variables. This finding is important to note, because even though our selection of regression variables did minimize mean absolute error, our metrics of spatial autocorrelation suggest that our model does have issues with generalizability when it is applied to homes in dissimilar locations. 

```{r Moran,results='hide', echo=FALSE, error=FALSE, message=FALSE, warning=FALSE,.asp=.5}

#Calculate observed and permuted Moran's I

Miami_test_newMI<-Miami_test_new[,c( "SalePrice.Error" )]
Miami_test_newMI <-na.omit(Miami_test_newMI)
coords.test <- st_centroid(st_geometry(Miami_test_newMI), of_largest_polygon=TRUE)
neighborList.test <- knn2nb(knearneigh(coords.test, 5))
spatialWeights.test <- nb2listw(neighborList.test, style="W")

moranTest <- moran.mc(Miami_test_newMI$SalePrice.Error, 
                      spatialWeights.test, nsim = 999)

# plot moran's i test

ggplot(as.data.frame(moranTest$res[c(1:999)]), aes(moranTest$res[c(1:999)])) +
  geom_histogram(binwidth = 0.01) +
  geom_vline(aes(xintercept = moranTest$statistic), colour = "#FA7800",size=1)+
  labs(title="Observed and permuted Moran's I",
       subtitle= "Observed Moran's I in orange",
       x="Moran's I",
       y="Count",
       caption="Figure 1.4.6") +
  plotTheme()

```

The maps below show our price predictions on on the training set next to price predictions from an unseen challenge set. The output appears to be consistent across both with no obvious anomalies.

```{r Predicted Maps,echo=FALSE, results='hide',error=FALSE, message=FALSE, warning=FALSE,fig.asp=.75}

# reloading clean data to test the challenge set

Miami_Houses <- 
  rbind(
    st_read("studentsData.geojson") %>%
      st_transform(st_crs('EPSG:6346')))
Miami_Houses <- st_set_crs(Miami_Houses, 6346)

# Load neighborhood boundaries

nhoodsmiami <- 
  rbind(
    st_read("https://opendata.arcgis.com/datasets/2f54a0cbd67046f2bd100fb735176e6c_0.geojson") %>%
      st_transform('EPSG:6346'))
nhoodsmiami <- st_set_crs(nhoodsmiami, 6346)

nhoodsmiamibeach <-
  rbind(st_read("neighborhoods/miamineighborhoods.shp")%>%
          st_transform('EPSG:6346'))
nhoodsmiamibeach <- st_set_crs(nhoodsmiamibeach,6346)

nhoodsmiami <- nhoodsmiami[2]
nhoodsmiamibeach <-nhoodsmiamibeach[1]%>%
  rename(LABEL = Name)
nhoodsmiami <-st_zm(nhoodsmiami, drop = TRUE, what = "ZM")
nhoodsmiamibeach <-st_zm(nhoodsmiamibeach, drop=TRUE, what = "ZM")
nhoods <- rbind(nhoodsmiami,nhoodsmiamibeach)%>%
  rename(neighborhood = LABEL)


# Create column with neighborhoods name

Miami_Houses <- st_join(Miami_Houses, nhoods, join = st_within)

# fix neighborhoods with limited entries

Miami_Houses$neighborhood2 <- ifelse(grepl("East Grove", Miami_Houses$neighborhood),Miami_Houses$neighborhood2<-"North Grove",
                                     ifelse(grepl("Bird Grove West", Miami_Houses$neighborhood), Miami_Houses$neighborhood2<-"Bird Grove East",
                                            ifelse(grepl("Bay Heights", Miami_Houses$neighborhood), Miami_Houses$neighborhood2<-"North Grove",
                                                   ifelse(grepl("Vizcaya", Miami_Houses$neighborhood), Miami_Houses$neighborhood2<-"North Grove",
                                                          ifelse(grepl("Fair Isle", Miami_Houses$neighborhood), Miami_Houses$neighborhood2<-"North Grove",Miami_Houses$neighborhood2<-Miami_Houses$neighborhood)))))

# create columns for house attributes

Miami_Houses$Pool <- ifelse(grepl("Pool", Miami_Houses$XF1), Miami_Houses$Pool<-"yes",
                            ifelse(grepl("Pool", Miami_Houses$XF2), Miami_Houses$Pool<-"yes",
                                   ifelse(grepl("Pool", Miami_Houses$XF3), Miami_Houses$Pool<-"yes",Miami_Houses$Pool<-"no")))

Miami_Houses$Patio <- ifelse(grepl("Patio", Miami_Houses$XF1), Miami_Houses$Patio<-"yes",
                             ifelse(grepl("Patio", Miami_Houses$XF2), Miami_Houses$Patio<-"yes",
                                    ifelse(grepl("Patio", Miami_Houses$XF3), Miami_Houses$Patio<-"yes",Miami_Houses$Patio<-"no")))

Miami_Houses$Carport <- ifelse(grepl("Carport", Miami_Houses$XF1), Miami_Houses$Carport<-"yes",
                               ifelse(grepl("Carport", Miami_Houses$XF2), Miami_Houses$Carport<-"yes",
                                      ifelse(grepl("Carport", Miami_Houses$XF3), Miami_Houses$Carport<-"yes",Miami_Houses$Carport<-"no")))

Miami_Houses$Whirlpool <- ifelse(grepl("Whirlpool", Miami_Houses$XF1), Miami_Houses$Whirlpool<-"yes",
                                 ifelse(grepl("Whirlpool", Miami_Houses$XF2), Miami_Houses$Whirlpool<-"yes",
                                        ifelse(grepl("Whirlpool", Miami_Houses$XF3), Miami_Houses$Whirlpool<-"yes",Miami_Houses$Whirlpool<-"no")))

Miami_Houses$Dock <- ifelse(grepl("Dock", Miami_Houses$XF1), Miami_Houses$Dock<-"yes",
                            ifelse(grepl("Dock", Miami_Houses$XF2), Miami_Houses$Dock<-"yes",
                                   ifelse(grepl("Dock", Miami_Houses$XF3), Miami_Houses$Dock<-"yes",Miami_Houses$Dock<-"no")))

# create training data

Miami_Training <- subset(Miami_Houses, toPredict %in% 0)

#create test data

`%nin%` = Negate(`%in%`)
Miami_Test <- subset(Miami_Houses,toPredict %nin% 0)

# finding average sale price of 5 nearest neighbors Test set

Miami_TestPPP <-as.ppp(st_centroid(Miami_Test))
Miami_TrainingPPP<-as.ppp(st_centroid(Miami_Training))

Miami_Test$nnHouse1 <- nncross(Miami_TestPPP, Miami_TrainingPPP, what="which",k=1)
Miami_Test$nnHouse2 <- nncross(Miami_TestPPP, Miami_TrainingPPP, what="which",k=2)
Miami_Test$nnHouse3 <- nncross(Miami_TestPPP, Miami_TrainingPPP, what="which",k=3)
Miami_Test$nnHouse4 <- nncross(Miami_TestPPP, Miami_TrainingPPP, what="which",k=4)
Miami_Test$nnHouse5 <- nncross(Miami_TestPPP, Miami_TrainingPPP, what="which",k=5)
Miami_Training$ID <- 1:2627

price <- function(data) {
  price1<-Miami_Training$SalePrice[Miami_Training$ID==data]
  return(price1)
}

Miami_Test$price1 <- lapply(Miami_Test$nnHouse1,price)
Miami_Test$price2 <- lapply(Miami_Test$nnHouse2,price)
Miami_Test$price3 <- lapply(Miami_Test$nnHouse3,price)
Miami_Test$price4 <- lapply(Miami_Test$nnHouse4,price)
Miami_Test$price5 <- lapply(Miami_Test$nnHouse5,price)

Miami_Test$price1 <- as.numeric(Miami_Test$price1)
Miami_Test$price2 <- as.numeric(Miami_Test$price2)
Miami_Test$price3 <- as.numeric(Miami_Test$price3)
Miami_Test$price4 <- as.numeric(Miami_Test$price4)
Miami_Test$price5 <- as.numeric(Miami_Test$price5)

Miami_Test$SalePriceAvg <- (Miami_Test$price1+Miami_Test$price2+Miami_Test$price3+Miami_Test$price4+Miami_Test$price5)/5

# finding average sale price of 5 nearest neighbors Training Set

Miami_Training$nnHouse1 <- nncross(Miami_TrainingPPP, Miami_TrainingPPP, what="which",k=2)
Miami_Training$nnHouse2 <- nncross(Miami_TrainingPPP, Miami_TrainingPPP, what="which",k=3)
Miami_Training$nnHouse3 <- nncross(Miami_TrainingPPP, Miami_TrainingPPP, what="which",k=4)
Miami_Training$nnHouse4 <- nncross(Miami_TrainingPPP, Miami_TrainingPPP, what="which",k=5)
Miami_Training$nnHouse5 <- nncross(Miami_TrainingPPP, Miami_TrainingPPP, what="which",k=6)

price <- function(data) {
  price1<-Miami_Training$SalePrice[Miami_Training$ID==data]
  return(price1)
}

Miami_Training$price1 <- lapply(Miami_Training$nnHouse1,price)
Miami_Training$price2 <- lapply(Miami_Training$nnHouse2,price)
Miami_Training$price3 <- lapply(Miami_Training$nnHouse3,price)
Miami_Training$price4 <- lapply(Miami_Training$nnHouse4,price)
Miami_Training$price5 <- lapply(Miami_Training$nnHouse5,price)

Miami_Training$price1 <- as.numeric(Miami_Training$price1)
Miami_Training$price2 <- as.numeric(Miami_Training$price2)
Miami_Training$price3 <- as.numeric(Miami_Training$price3)
Miami_Training$price4 <- as.numeric(Miami_Training$price4)
Miami_Training$price5 <- as.numeric(Miami_Training$price5)

Miami_Training$SalePriceAvg <- (Miami_Training$price1+Miami_Training$price2+Miami_Training$price3+Miami_Training$price4+Miami_Training$price5)/5

# create the final model

reg.training <- lm(SalePrice ~ ., data = st_drop_geometry(Miami_Training) %>% 
                      dplyr::select(SalePrice,neighborhood2,SalePriceAvg,Dock,Zoning))

# test challenge set

Miami_Test <-
  Miami_Test %>%
  mutate(SalePrice.Predict = predict(reg.training, Miami_Test),
         SalePrice.Error = SalePrice.Predict - SalePrice,
         SalePrice.AbsError = abs(SalePrice.Predict - SalePrice),
         SalePrice.APE = (abs(SalePrice.Predict - SalePrice)) / SalePrice.Predict)%>%
  filter(SalePrice < 5000000)

Miami_Testmap <- Miami_Test[,c("SalePrice.Predict")]
Miami_Testmap$Type <- "Challenge Set"

# predicting training set

reg.training <- lm(SalePrice ~ ., data = st_drop_geometry(Miami_Training) %>% 
                      dplyr::select(SalePrice,neighborhood2,SalePriceAvg,Dock,Zoning))

Miami_Training <-
  Miami_Training %>%
  mutate(SalePrice.Predict = predict(reg.training, Miami_Training),
         SalePrice.Error = SalePrice.Predict - SalePrice,
         SalePrice.AbsError = abs(SalePrice.Predict - SalePrice),
         SalePrice.APE = (abs(SalePrice.Predict - SalePrice)) / SalePrice.Predict)%>%
  filter(SalePrice < 5000000)
Miami_Trainingmap <- Miami_Training[,c("SalePrice.Predict")]
Miami_Trainingmap$Type <- "Training Set"

#plot maps of predictions

MAEPrediction <- rbind(Miami_Testmap, Miami_Trainingmap)

ggplot() +
  geom_sf(data = nhoods, fill = "grey40") +
  geom_sf(data =st_centroid(MAEPrediction), aes(colour = q5(SalePrice.Predict)), 
          show.legend = "point", size = .5) +
  scale_colour_manual(values = palette5,
                      labels=qBr(Miami_Training,"SalePrice.Predict"),
                      name="Quintile\nBreaks") +
  labs(title="Predicted Sale Prices of Training and Challenge Sets",
  subtitle="Miami, Florida", 
       caption= "Figure 1.4.7") +
  facet_wrap(~Type)+
  mapTheme()

```

## Generalizability Across Neighbohood, Race, and Income

The graphic below plots the mean average percentage error of home predictions by neighborhood, which further helps us understand where our model is most accurate. Large errors (represented by an orange color) are highest along the coast and especially high in several neighborhoods in Miami Beach. These same neighborhoods feature very expensive homes, which we hypothesize is a major driver of their high errors. Our model is clearly missing a key feature in predicting the most expensive home values.

```{r Generalizability,echo=FALSE, error=FALSE, message=FALSE, warning=FALSE,fig.asp=.75}

#Assess generalizability of the neighborhood model

nhoods$neighborhood2 = nhoods$neighborhood

st_drop_geometry(Miami_test_new) %>%
  group_by(Regression, neighborhood) %>%
  summarize(mean.MAPE = mean(SalePrice.APE, na.rm = T)) %>%
  ungroup() %>% 
  left_join(nhoods) %>%
  st_sf() %>%
  ggplot() + 
  geom_sf(data = nhoods, fill = "grey40") +
  geom_sf(aes(fill = mean.MAPE)) +
  geom_sf(data = Miami_test_new, colour = "black", size = .5)+
  scale_fill_gradient(low = palette5[1], high = palette5[5],
                      name = "MAPE") +
  labs(title = "Mean test set MAPE by neighborhood",subtitle="Miami, Florida", 
       caption= "Figure 1.4.8") +
  mapTheme()

```

```{r MAPE Plot,echo=FALSE, error=FALSE, message=FALSE, warning=FALSE,fig.asp=.5}

#Plot MAPE by neighborhood as function of mean price by neighborhood

MAPE.Summary <- 
  st_drop_geometry(Miami_test_new) %>%
  group_by(neighborhood) %>%
  summarize(mean.MAPE = mean(SalePrice.APE),
            mean.Price =mean(SalePrice))
ggplot(data=MAPE.Summary, aes(mean.Price,mean.MAPE)) +
  geom_point()+
  geom_smooth(method=lm, se=FALSE)+
  labs(title="MAPE by neighborhood as a function of mean price by neighborhood", x="Mean Price", y="MAPE", 
       caption= "Figure 1.4.9")+
  plotTheme()

```

A good model is generalizable - that is it predicts accurately across different sets of data that it has not seen before. In order to assess the generalizability of the model, we broke out Miami census tracts into two groups according to Race and Income. The first comparison tests MAPE across majority white and majority non-white tracts, and finds that the MAPE remains constant. This suggests that there is no obvious error in the model that correlates with racial composition of neighborhoods. The second tests MAPE across high-income and low-income tracts, and finds a somewhat sizable difference between high-income tracts and low-income tracts. This finding suggests that the model is not fully controlling for some factor related to income, and that the model appears to produce more errors when evaluating high-income tracts.

```{r Census Data,echo=FALSE, error=FALSE, message=FALSE, warning=FALSE,results='hide',fig.asp=.75}

#Import census data to assess generalizability

tracts17 <- 
  get_acs(geography = "tract", variables = c("B01001_001E","B01001A_001E","B06011_001"), 
          year=2017, state=12, county="Miami-Dade County", geometry=T, output="wide") %>%
  st_transform('EPSG:6346')  %>%
  rename(TotalPop = B01001_001E,
         NumberWhites = B01001A_001E,
         Median_Income = B06011_001E) %>%
  mutate(percentWhite = NumberWhites / TotalPop,
         raceContext = ifelse(percentWhite > .5, "Majority White", "Majority Non-White"),
         incomeContext = ifelse(Median_Income > 32322, "High Income", "Low Income"))

miami <- st_union(nhoods)
tracts17.miami.intersect <- st_intersects(miami, tracts17)
tracts17miami <- tracts17[tracts17.miami.intersect[[1]],]

# map race and income boundaries

grid.arrange(ncol = 2,
             ggplot() + geom_sf(data = na.omit(tracts17miami), aes(fill = raceContext)) +
               scale_fill_manual(values = c("#25CB10", "#FA7800"), name="Race Context") +
               labs(title = "Race Context", caption= "Figure 1.4.10") +
               mapTheme() + theme(legend.position="bottom"), 
             ggplot() + geom_sf(data = na.omit(tracts17miami), aes(fill = incomeContext)) +
               scale_fill_manual(values = c("#25CB10", "#FA7800"), name="Income Context") +
               labs(title = "Income Context",caption= "Figure 1.4.11") +
               mapTheme() + theme(legend.position="bottom"))

```
```{r MAPE by Race,echo=FALSE, error=FALSE, message=FALSE, warning=FALSE,fig.asp=.5}

#Test MAPE by racial context

sum1<-st_join(Miami_test_new, tracts17) %>% 
  group_by(Regression, raceContext) %>%
  summarize(mean.MAPE = scales::percent(mean(SalePrice.APE, na.rm = T)))

# Create table of MAPE by racial context

sum1<-sum1[,c("raceContext","mean.MAPE")]

kable(st_drop_geometry(sum1),format = "html", caption = "Test set MAPE by neighborhood racial context", col.names = c('', '')) %>% 
  kable_styling() %>%
  footnote(general_title = "\n",
           general = "Table 1.4.4")
```
```{r Mape by Income,echo=FALSE, error=FALSE, message=FALSE, warning=FALSE,fig.asp=.5}

#Test MAPE by income context

sum2<-st_join(Miami_test_new, tracts17) %>% 
  filter(!is.na(incomeContext)) %>%
  group_by(Regression, incomeContext) %>%
  summarize(mean.MAPE = scales::percent(mean(SalePrice.APE, na.rm = T))) %>%
  st_drop_geometry()
sum2<-sum2[,c("incomeContext","mean.MAPE")]

# create table of MAPE of income context

kable(sum2,format = "html", caption = "Test set MAPE by neighborhood income context", col.names = c('', '')) %>% 
  kable_styling() %>%
  footnote(general_title = "\n",
           general = "Table 1.4.5")

```

# 1.5 Discussion

On the whole, our model appears to predict prices reasonably well but is far from an ideal model. Analyzing the results of our model, revealed several aspects that appear problematic and warrant further consideration:

## Variable Selection

Interestingly, the variable set we found to produce the lowest mean absolute error consisted mostly of spatial structure features. Only the variable "Dock" relates to the physical nature of the home in question. To our surprise, other asset level variables such as square footage and number of bedrooms did not reduce error when included in the model. We hypothesize that neighborhood and average sale price of nearby homes are highly correlated with home size and possibly causing colinearity issues when these variables are included. However, this leaves our model ill-equipped to accurately predict in situations where a house varies considerably from those nearby it, for example when valuing an unusually small house in a neighborhood of large homes. 

## Error vs. Home Price

Our graph of predicted versus actual prices reveals the tendency of our model to increasingly under-predict as sale values increase. We hypothesize that the source of this error may be rooted in the variable selection of our model, which as discussed above lacks asset level features. Because the model cannot see square footage or bedroom count for example, it would plausibly under-predict the value of very expensive homes that are larger than others in close proximity. The significantly higher MAPE that we observe in high-income vs. lower-income neighborhoods is also likely caused by the model's greater error when predicting very expensive homes. 

## Spatial Error

Our Moran's I test reveals that there is a non-trivial amount of spatial autocorrelation of our model's errors that cannot be explained by random chance. Although completely eliminating spatial error may be challenging, our value was not ideal. We would be interested in seeing whether this spatial error could be resolved by addressing some of the more glaring problems relating to variable selection. 

# 1.6 Conclusion

Our model is imperfect, but it does offer a significant improvement over predictions based solely on the variables in the original dataset. This tells us that local spatial data does serve as a valuable predictive tool in accurately valuing homes. We would be interested in discussing our model with Zillow to determine whether they have considered the same spatial variables we did, and whether there are any that might improve their models. However, with a mean absolute error of over $200,000, or over 20% of average home sale value, our model is unlikely to outperform the Zestimate in minimizing errors.  

There is clearly significant room for our model to improve, and to do so we would could consider:
1. Further analysis of apparent errors in the model. Our model seems to consistently under-predict at higher home values, particularly at price points higher than 2.5MM. 
2. Inclusion of more asset level variables. In order to be more accurate, it seems likely that the model would need to be able to identify homes that have physical features that differ greatly from others located nearby.
3. Assessment of other spatial data. We also attempted to include several other variables that exceeded our technical ability to implement, such as WalkScore and TransitScore. 



